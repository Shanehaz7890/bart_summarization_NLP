{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BART Text Summarization Project\n",
        "\n",
        "This notebook implements a complete BART (Bidirectional and Auto-Regressive Transformers) text summarization pipeline following the methodology from the midterm presentation slides.\n",
        "\n",
        "## Methodology Overview\n",
        "1. **Collect and Select Dataset** - CNN/DailyMail dataset\n",
        "2. **Preprocess text** - Tokenization and cleaning\n",
        "3. **Fine-tune BART** - Configure hyperparameters (max_len, min_len, temp, num_beams, length_penalty)\n",
        "4. **Generate summaries** - Use fine-tuned model to create summaries\n",
        "5. **Evaluate** - Use ROUGE scores to assess model performance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shane\\OneDrive\\KSU Schoolwork\\Fall 25\\CS 4742 Natural Language Processing\\Final Pres\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    BartForConditionalGeneration,\n",
        "    BartTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "from datasets import Dataset\n",
        "from rouge_score import rouge_scorer\n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Set up device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Dataset Loading and Exploration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading datasets...\n",
            "Training samples: 287113\n",
            "Validation samples: 13368\n",
            "Test samples: 11490\n",
            "\n",
            "Dataset columns: ['id', 'article', 'highlights']\n",
            "\n",
            "First few rows:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>article</th>\n",
              "      <th>highlights</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0001d1afc246a7964130f43ae940af6bc6c57f01</td>\n",
              "      <td>By . Associated Press . PUBLISHED: . 14:11 EST...</td>\n",
              "      <td>Bishop John Folda, of North Dakota, is taking ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0002095e55fcbd3a2f366d9bf92a95433dc305ef</td>\n",
              "      <td>(CNN) -- Ralph Mata was an internal affairs li...</td>\n",
              "      <td>Criminal complaint: Cop used his role to help ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00027e965c8264c35cc1bc55556db388da82b07f</td>\n",
              "      <td>A drunk driver who killed a young woman in a h...</td>\n",
              "      <td>Craig Eccleston-Todd, 27, had drunk at least t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0002c17436637c4fe1837c935c04de47adb18e9a</td>\n",
              "      <td>(CNN) -- With a breezy sweep of his pen Presid...</td>\n",
              "      <td>Nina dos Santos says Europe must be ready to a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0003ad6ef0c37534f80b55b4235108024b407f0b</td>\n",
              "      <td>Fleetwood are the only team still to have a 10...</td>\n",
              "      <td>Fleetwood top of League One after 2-0 win at S...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         id  \\\n",
              "0  0001d1afc246a7964130f43ae940af6bc6c57f01   \n",
              "1  0002095e55fcbd3a2f366d9bf92a95433dc305ef   \n",
              "2  00027e965c8264c35cc1bc55556db388da82b07f   \n",
              "3  0002c17436637c4fe1837c935c04de47adb18e9a   \n",
              "4  0003ad6ef0c37534f80b55b4235108024b407f0b   \n",
              "\n",
              "                                             article  \\\n",
              "0  By . Associated Press . PUBLISHED: . 14:11 EST...   \n",
              "1  (CNN) -- Ralph Mata was an internal affairs li...   \n",
              "2  A drunk driver who killed a young woman in a h...   \n",
              "3  (CNN) -- With a breezy sweep of his pen Presid...   \n",
              "4  Fleetwood are the only team still to have a 10...   \n",
              "\n",
              "                                          highlights  \n",
              "0  Bishop John Folda, of North Dakota, is taking ...  \n",
              "1  Criminal complaint: Cop used his role to help ...  \n",
              "2  Craig Eccleston-Todd, 27, had drunk at least t...  \n",
              "3  Nina dos Santos says Europe must be ready to a...  \n",
              "4  Fleetwood top of League One after 2-0 win at S...  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load datasets from CSV files\n",
        "print(\"Loading datasets...\")\n",
        "train_df = pd.read_csv('cnn_dailymail/train.csv')\n",
        "val_df = pd.read_csv('cnn_dailymail/validation.csv')\n",
        "test_df = pd.read_csv('cnn_dailymail/test.csv')\n",
        "\n",
        "print(f\"Training samples: {len(train_df)}\")\n",
        "print(f\"Validation samples: {len(val_df)}\")\n",
        "print(f\"Test samples: {len(test_df)}\")\n",
        "\n",
        "# Display dataset structure\n",
        "print(\"\\nDataset columns:\", train_df.columns.tolist())\n",
        "print(\"\\nFirst few rows:\")\n",
        "train_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Set Statistics:\n",
            "Article length - Mean: 691.9, Median: 632.0, Max: 2347\n",
            "Summary length - Mean: 51.6, Median: 48.0, Max: 1296\n",
            "\n",
            "================================================================================\n",
            "Sample Article and Summary:\n",
            "================================================================================\n",
            "\n",
            "Article ID: 0001d1afc246a7964130f43ae940af6bc6c57f01\n",
            "\n",
            "Article (first 500 chars):\n",
            "By . Associated Press . PUBLISHED: . 14:11 EST, 25 October 2013 . | . UPDATED: . 15:36 EST, 25 October 2013 . The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus in late September and early October. The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion. Bishop John Folda (pictured) of the Fargo Catholic Diocese in N...\n",
            "\n",
            "Summary:\n",
            "Bishop John Folda, of North Dakota, is taking time off after being diagnosed .\n",
            "He contracted the infection through contaminated food in Italy .\n",
            "Church members in Fargo, Grand Forks and Jamestown could have been exposed .\n"
          ]
        }
      ],
      "source": [
        "# Explore dataset characteristics\n",
        "def get_text_lengths(text):\n",
        "    \"\"\"Get word and character counts for text\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return 0, 0\n",
        "    words = text.split()\n",
        "    return len(words), len(text)\n",
        "\n",
        "# Calculate statistics\n",
        "train_article_lengths = train_df['article'].apply(lambda x: get_text_lengths(x)[0])\n",
        "train_summary_lengths = train_df['highlights'].apply(lambda x: get_text_lengths(x)[0])\n",
        "\n",
        "print(\"Training Set Statistics:\")\n",
        "print(f\"Article length - Mean: {train_article_lengths.mean():.1f}, Median: {train_article_lengths.median():.1f}, Max: {train_article_lengths.max()}\")\n",
        "print(f\"Summary length - Mean: {train_summary_lengths.mean():.1f}, Median: {train_summary_lengths.median():.1f}, Max: {train_summary_lengths.max()}\")\n",
        "\n",
        "# Display sample article and summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Sample Article and Summary:\")\n",
        "print(\"=\"*80)\n",
        "sample_idx = 0\n",
        "print(f\"\\nArticle ID: {train_df.iloc[sample_idx]['id']}\")\n",
        "print(f\"\\nArticle (first 500 chars):\\n{train_df.iloc[sample_idx]['article'][:500]}...\")\n",
        "print(f\"\\nSummary:\\n{train_df.iloc[sample_idx]['highlights']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Text Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenizer for facebook/bart-large-cnn...\n",
            "Vocabulary size: 50265\n",
            "Max model length: 1000000000000000019884624838656\n"
          ]
        }
      ],
      "source": [
        "# Load BART tokenizer\n",
        "model_name = 'facebook/bart-large-cnn'  # Pre-trained on CNN/DailyMail\n",
        "print(f\"Loading tokenizer for {model_name}...\")\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Display tokenizer info\n",
        "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
        "print(f\"Max model length: {tokenizer.model_max_length}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing function defined.\n"
          ]
        }
      ],
      "source": [
        "# Text cleaning function\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean and normalize text\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "# Preprocessing function for tokenization\n",
        "def preprocess_function(examples, max_input_length=1024, max_target_length=142):\n",
        "    \"\"\"\n",
        "    Preprocess articles and highlights for BART\n",
        "    - max_input_length: Maximum length for articles (BART's limit is 1024)\n",
        "    - max_target_length: Maximum length for summaries (typical for CNN/DailyMail is 142)\n",
        "    \"\"\"\n",
        "    # Clean text\n",
        "    articles = [clean_text(article) for article in examples['article']]\n",
        "    highlights = [clean_text(highlight) for highlight in examples['highlights']]\n",
        "    \n",
        "    # Tokenize inputs (articles)\n",
        "    model_inputs = tokenizer(\n",
        "        articles,\n",
        "        max_length=max_input_length,\n",
        "        truncation=True,\n",
        "        padding='max_length'\n",
        "    )\n",
        "    \n",
        "    # Tokenize targets (highlights/summaries)\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            highlights,\n",
        "            max_length=max_target_length,\n",
        "            truncation=True,\n",
        "            padding='max_length'\n",
        "        )\n",
        "    \n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "    return model_inputs\n",
        "\n",
        "print(\"Preprocessing function defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converting to HuggingFace Datasets format...\n",
            "Preprocessing datasets (this may take a few minutes)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 287113/287113 [12:37<00:00, 379.21 examples/s]\n",
            "Map: 100%|██████████| 13368/13368 [00:36<00:00, 369.93 examples/s]\n",
            "Map: 100%|██████████| 11490/11490 [00:31<00:00, 366.04 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing complete!\n",
            "Train dataset size: 287113\n",
            "Validation dataset size: 13368\n",
            "Test dataset size: 11490\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Convert DataFrames to HuggingFace Datasets\n",
        "print(\"Converting to HuggingFace Datasets format...\")\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# Apply preprocessing\n",
        "print(\"Preprocessing datasets (this may take a few minutes)...\")\n",
        "train_dataset = train_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names\n",
        ")\n",
        "\n",
        "val_dataset = val_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=val_dataset.column_names\n",
        ")\n",
        "\n",
        "# For test set, we'll keep the original text for evaluation\n",
        "test_dataset_processed = test_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=test_dataset.column_names\n",
        ")\n",
        "\n",
        "print(\"Preprocessing complete!\")\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset_processed)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data collator created.\n"
          ]
        }
      ],
      "source": [
        "# Create data collator for sequence-to-sequence tasks\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model_name,\n",
        "    padding=True\n",
        ")\n",
        "\n",
        "print(\"Data collator created.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: BART Model Fine-tuning\n",
        "\n",
        "### Hyperparameter Configuration\n",
        "Based on the methodology, we'll configure:\n",
        "- `max_length`: Maximum length for generated summaries\n",
        "- `min_length`: Minimum length for generated summaries  \n",
        "- `temperature`: Sampling temperature\n",
        "- `num_beams`: Number of beams for beam search\n",
        "- `length_penalty`: Length penalty for beam search\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generation hyperparameters configured:\n",
            "  max_length: 142\n",
            "  min_length: 56\n",
            "  temperature: 1.0\n",
            "  num_beams: 4\n",
            "  length_penalty: 2.0\n",
            "  no_repeat_ngram_size: 3\n",
            "  early_stopping: True\n"
          ]
        }
      ],
      "source": [
        "# Configuration for generation hyperparameters\n",
        "GENERATION_CONFIG = {\n",
        "    'max_length': 142,      # Maximum summary length (typical for CNN/DailyMail)\n",
        "    'min_length': 56,       # Minimum summary length\n",
        "    'temperature': 1.0,     # Temperature for sampling (1.0 = deterministic)\n",
        "    'num_beams': 4,         # Number of beams for beam search\n",
        "    'length_penalty': 2.0,  # Length penalty (higher = longer summaries)\n",
        "    'no_repeat_ngram_size': 3,  # Prevent repetition\n",
        "    'early_stopping': True\n",
        "}\n",
        "\n",
        "print(\"Generation hyperparameters configured:\")\n",
        "for key, value in GENERATION_CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading BART model: facebook/bart-large-cnn...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded on cpu\n",
            "Model parameters: 406.3M\n"
          ]
        }
      ],
      "source": [
        "# Load pre-trained BART model\n",
        "print(f\"Loading BART model: {model_name}...\")\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "model.to(device)\n",
        "print(f\"Model loaded on {device}\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.1f}M\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training arguments configured:\n",
            "  Epochs: 3\n",
            "  Batch size: 4\n",
            "  Gradient accumulation: 4\n",
            "  Effective batch size: 16\n",
            "  Learning rate: 3e-05\n"
          ]
        }
      ],
      "source": [
        "# Configure training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./bart-cnn-dailymail',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,  # Adjust based on GPU memory\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=4,  # Effective batch size = 4 * 4 = 16\n",
        "    warmup_steps=500,\n",
        "    learning_rate=3e-5,\n",
        "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\"  # Disable wandb/tensorboard\n",
        ")\n",
        "\n",
        "print(\"Training arguments configured:\")\n",
        "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  Learning rate: {training_args.learning_rate}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainer created. Ready to start fine-tuning!\n"
          ]
        }
      ],
      "source": [
        "# Create Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(\"Trainer created. Ready to start fine-tuning!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting fine-tuning...\n",
            "This may take several hours depending on your hardware.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11' max='53835' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   11/53835 04:16 < 426:02:57, 0.04 it/s, Epoch 0.00/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Start training\n",
        "print(\"Starting fine-tuning...\")\n",
        "print(\"This may take several hours depending on your hardware.\")\n",
        "trainer.train()\n",
        "\n",
        "# Save the final model\n",
        "trainer.save_model('./bart-cnn-dailymail/final_model')\n",
        "tokenizer.save_pretrained('./bart-cnn-dailymail/final_model')\n",
        "print(\"\\nFine-tuning complete! Model saved to './bart-cnn-dailymail/final_model'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Fine-tuned Model (if resuming)\n",
        "\n",
        "If you've already fine-tuned the model and want to load it:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to load a previously fine-tuned model\n",
        "# model_path = './bart-cnn-dailymail/final_model'\n",
        "# model = BartForConditionalGeneration.from_pretrained(model_path)\n",
        "# tokenizer = BartTokenizer.from_pretrained(model_path)\n",
        "# model.to(device)\n",
        "# print(f\"Loaded fine-tuned model from {model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Summary Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_summary(model, tokenizer, article, generation_config):\n",
        "    \"\"\"\n",
        "    Generate a summary for a given article\n",
        "    \n",
        "    Args:\n",
        "        model: Fine-tuned BART model\n",
        "        tokenizer: BART tokenizer\n",
        "        article: Input article text\n",
        "        generation_config: Dictionary with generation hyperparameters\n",
        "    \n",
        "    Returns:\n",
        "        Generated summary text\n",
        "    \"\"\"\n",
        "    # Clean article text\n",
        "    article = clean_text(article)\n",
        "    \n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(\n",
        "        article,\n",
        "        max_length=1024,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        return_tensors='pt'\n",
        "    ).to(device)\n",
        "    \n",
        "    # Generate summary\n",
        "    with torch.no_grad():\n",
        "        summary_ids = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            max_length=generation_config['max_length'],\n",
        "            min_length=generation_config['min_length'],\n",
        "            num_beams=generation_config['num_beams'],\n",
        "            length_penalty=generation_config['length_penalty'],\n",
        "            temperature=generation_config['temperature'],\n",
        "            no_repeat_ngram_size=generation_config['no_repeat_ngram_size'],\n",
        "            early_stopping=generation_config['early_stopping']\n",
        "        )\n",
        "    \n",
        "    # Decode summary\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "print(\"Summary generation function defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate summaries for a sample of test articles\n",
        "print(\"Generating summaries for test set...\")\n",
        "print(\"This may take a while depending on the number of test samples...\")\n",
        "\n",
        "# Use a subset for demonstration (you can change this to use the full test set)\n",
        "num_test_samples = min(100, len(test_df))  # Generate for first 100 test samples\n",
        "test_subset = test_df.head(num_test_samples)\n",
        "\n",
        "generated_summaries = []\n",
        "reference_summaries = []\n",
        "\n",
        "for idx, row in tqdm(test_subset.iterrows(), total=len(test_subset), desc=\"Generating summaries\"):\n",
        "    article = row['article']\n",
        "    reference = row['highlights']\n",
        "    \n",
        "    # Generate summary\n",
        "    generated = generate_summary(model, tokenizer, article, GENERATION_CONFIG)\n",
        "    \n",
        "    generated_summaries.append(generated)\n",
        "    reference_summaries.append(reference)\n",
        "\n",
        "print(f\"\\nGenerated {len(generated_summaries)} summaries!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display sample generated summaries\n",
        "print(\"=\"*80)\n",
        "print(\"Sample Generated Summaries\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "num_samples_to_show = 3\n",
        "for i in range(min(num_samples_to_show, len(generated_summaries))):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Sample {i+1}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"\\nArticle (first 300 chars):\\n{test_subset.iloc[i]['article'][:300]}...\")\n",
        "    print(f\"\\n{'─'*80}\")\n",
        "    print(f\"Reference Summary:\\n{reference_summaries[i]}\")\n",
        "    print(f\"\\n{'─'*80}\")\n",
        "    print(f\"Generated Summary:\\n{generated_summaries[i]}\")\n",
        "    print(f\"\\n{'='*80}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Evaluation Metrics\n",
        "\n",
        "### ROUGE Scores\n",
        "\n",
        "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) scores measure summary quality against reference texts. We'll calculate:\n",
        "- **ROUGE-1**: Overlap of unigrams (single words)\n",
        "- **ROUGE-2**: Overlap of bigrams (word pairs)\n",
        "- **ROUGE-L**: Longest common subsequence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize ROUGE scorer\n",
        "rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "def calculate_rouge_scores(generated_summaries, reference_summaries):\n",
        "    \"\"\"\n",
        "    Calculate ROUGE-1, ROUGE-2, and ROUGE-L scores\n",
        "    \n",
        "    Args:\n",
        "        generated_summaries: List of generated summary texts\n",
        "        reference_summaries: List of reference summary texts\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with average ROUGE scores\n",
        "    \"\"\"\n",
        "    rouge1_scores = []\n",
        "    rouge2_scores = []\n",
        "    rougeL_scores = []\n",
        "    \n",
        "    for gen, ref in zip(generated_summaries, reference_summaries):\n",
        "        scores = rouge_scorer_obj.score(ref, gen)\n",
        "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
        "        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
        "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
        "    \n",
        "    return {\n",
        "        'rouge1': {\n",
        "            'precision': np.mean([rouge_scorer_obj.score(ref, gen)['rouge1'].precision \n",
        "                                  for gen, ref in zip(generated_summaries, reference_summaries)]),\n",
        "            'recall': np.mean([rouge_scorer_obj.score(ref, gen)['rouge1'].recall \n",
        "                              for gen, ref in zip(generated_summaries, reference_summaries)]),\n",
        "            'fmeasure': np.mean(rouge1_scores)\n",
        "        },\n",
        "        'rouge2': {\n",
        "            'precision': np.mean([rouge_scorer_obj.score(ref, gen)['rouge2'].precision \n",
        "                                  for gen, ref in zip(generated_summaries, reference_summaries)]),\n",
        "            'recall': np.mean([rouge_scorer_obj.score(ref, gen)['rouge2'].recall \n",
        "                              for gen, ref in zip(generated_summaries, reference_summaries)]),\n",
        "            'fmeasure': np.mean(rouge2_scores)\n",
        "        },\n",
        "        'rougeL': {\n",
        "            'precision': np.mean([rouge_scorer_obj.score(ref, gen)['rougeL'].precision \n",
        "                                  for gen, ref in zip(generated_summaries, reference_summaries)]),\n",
        "            'recall': np.mean([rouge_scorer_obj.score(ref, gen)['rougeL'].recall \n",
        "                              for gen, ref in zip(generated_summaries, reference_summaries)]),\n",
        "            'fmeasure': np.mean(rougeL_scores)\n",
        "        }\n",
        "    }\n",
        "\n",
        "print(\"ROUGE calculation function defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate ROUGE scores\n",
        "print(\"Calculating ROUGE scores...\")\n",
        "rouge_scores = calculate_rouge_scores(generated_summaries, reference_summaries)\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ROUGE Evaluation Results\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nNumber of samples evaluated: {len(generated_summaries)}\")\n",
        "print(f\"\\n{'Metric':<15} {'Precision':<12} {'Recall':<12} {'F-Measure':<12}\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"{'ROUGE-1':<15} {rouge_scores['rouge1']['precision']:<12.4f} {rouge_scores['rouge1']['recall']:<12.4f} {rouge_scores['rouge1']['fmeasure']:<12.4f}\")\n",
        "print(f\"{'ROUGE-2':<15} {rouge_scores['rouge2']['precision']:<12.4f} {rouge_scores['rouge2']['recall']:<12.4f} {rouge_scores['rouge2']['fmeasure']:<12.4f}\")\n",
        "print(f\"{'ROUGE-L':<15} {rouge_scores['rougeL']['precision']:<12.4f} {rouge_scores['rougeL']['recall']:<12.4f} {rouge_scores['rougeL']['fmeasure']:<12.4f}\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Human Evaluation\n",
        "\n",
        "As mentioned in the presentation slides, human evaluation is another important metric for assessing subjective quality in summarization tasks. While automated metrics like ROUGE provide quantitative measures, human evaluation can assess:\n",
        "- **Coherence**: How well the summary flows and makes sense\n",
        "- **Fluency**: How natural and readable the summary is\n",
        "- **Relevance**: How well the summary captures the key information\n",
        "- **Coverage**: How comprehensively the summary covers the article\n",
        "\n",
        "For this project, we focus on automated ROUGE evaluation. Human evaluation would typically involve having human annotators rate the generated summaries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook has implemented a complete BART text summarization pipeline:\n",
        "\n",
        "1. ✅ **Dataset Loading**: Loaded and explored the CNN/DailyMail dataset\n",
        "2. ✅ **Preprocessing**: Tokenized and prepared the data for training\n",
        "3. ✅ **Fine-tuning**: Fine-tuned BART with configurable hyperparameters\n",
        "4. ✅ **Generation**: Generated summaries using the fine-tuned model\n",
        "5. ✅ **Evaluation**: Calculated ROUGE scores to assess model performance\n",
        "\n",
        "The fine-tuned model can now be used to generate summaries for new articles!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
